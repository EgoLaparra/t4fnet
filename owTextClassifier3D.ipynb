{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text classifier\n",
    "\n",
    "This LSTM classifier operates over tweets to classify Twitter account users as Overweight or Not Overweight.\n",
    "\n",
    "Each tweet is its own entry that the LSTM operates over, so we need a 3D data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "np.random.seed(947) # for reproducibility\n",
    "import pickle as pkl\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad3d(sequences, maxtweets=None, maxlen=None, dtype='int32',\n",
    "          padding='pre', truncating='pre', value=0.):\n",
    "    '''\n",
    "        # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxtweets, maxlen)\n",
    "    '''\n",
    "    nb_samples = len(sequences)\n",
    "    if maxtweets is not None:\n",
    "        mt = maxtweets\n",
    "    else:\n",
    "        mt = find_most_tweets(sequences)\n",
    "    #print('maximum # tweets: %i' % mt)\n",
    "    \n",
    "    if maxlen is not None:\n",
    "        ml = maxlen\n",
    "    else:\n",
    "        ml = find_longest(sequences)\n",
    "    #print('maximum tweet length: %i' % ml)\n",
    "        \n",
    "    x = (np.ones((nb_samples, mt, ml)) * value).astype(dtype)\n",
    "    #print('x shape: ', x.shape)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue # no tweets\n",
    "        x[idx, :min(mt,len(s))] = sequence.pad_sequences(s[:mt], ml, dtype, padding, truncating, value)\n",
    "    return x\n",
    "\n",
    "def find_most_tweets(x):\n",
    "    currmax = 0\n",
    "    for account in x:\n",
    "        currlen = len(account)\n",
    "        if currlen > currmax:\n",
    "            currmax = currlen\n",
    "    return currmax\n",
    "\n",
    "def find_longest(x):\n",
    "    currmax = 0\n",
    "    for account in x:\n",
    "        for tweet in account:\n",
    "            currlen = len(tweet)\n",
    "            if currlen > currmax:\n",
    "                currmax = currlen\n",
    "    return currmax\n",
    "\n",
    "def cap_words(x, nb_words, oov=2):\n",
    "    return [[[oov if w >= nb_words else w for w in z] for z in y] for y in x]\n",
    "\n",
    "def skip_n(x, n, oov=2):\n",
    "    return [[[oov if w < n else w for w in z] for z in y] for y in x]\n",
    "\n",
    "def cap_length(x, maxlen):\n",
    "    return [[z[:maxlen] for z in y] for y in x]\n",
    "\n",
    "def push_indices(x, start, index_from):\n",
    "    if start is not None:\n",
    "        return [[[start] + [w + index_from for w in z] for z in y] for y in x]\n",
    "    elif index_from:\n",
    "        return [[[w + index_from for w in z] for z in y] for y in x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def load_data(path='ow3d.pkl', nb_words=None, skip_top=0,\n",
    "              maxlen=None, seed=113, start=1, oov=2, index_from=3):\n",
    "    '''\n",
    "    # Arguments\n",
    "        path: where the data is stored (in '.')\n",
    "        nb_words: max number of words to include. Words are ranked\n",
    "            by how often they occur (in the training set) and only\n",
    "            the most frequent words are kept\n",
    "        skip_top: skip the top N most frequently occuring words\n",
    "            (which may not be informative).\n",
    "        maxlen: truncate sequences after this length.\n",
    "        seed: random seed for sample shuffling.\n",
    "        start_char: The start of a sequence will be marked with this character.\n",
    "            Set to 1 because 0 is usually the padding character.\n",
    "        oov: words that were cut out because of the `nb_words`\n",
    "            or `skip_top` limit will be replaced with this character.\n",
    "        index_from: index actual words with this index and higher.\n",
    "\n",
    "    Note that the 'out of vocabulary' character is only used for\n",
    "    words that were present in the training set but are not included\n",
    "    because they're not making the `nb_words` cut here.\n",
    "    Words that were not seen in the training set but are in the test set\n",
    "    have simply been skipped. See preprocessText3D\n",
    "    \n",
    "    Adapted from keras.datasets.imdb.py by FranÃ§ois Chollet\n",
    "    '''\n",
    "    \n",
    "    if path.endswith(\".gz\"):\n",
    "        f = gzip.open(path, 'rb')\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "\n",
    "    (x_pos, y_pos) = pkl.load(f)\n",
    "    (x_neg, y_neg) = pkl.load(f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # randomize datum order\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(x_pos)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y_pos)\n",
    "\n",
    "    np.random.seed(seed * 2)\n",
    "    np.random.shuffle(x_neg)\n",
    "    np.random.seed(seed * 2)\n",
    "    np.random.shuffle(y_neg)\n",
    "    \n",
    "    # keep maxlen words of each tweet\n",
    "    if maxlen is not None:\n",
    "        x_pos = cap_length(x_pos, maxlen)\n",
    "        x_neg = cap_length(x_neg, maxlen)\n",
    "\n",
    "    # cut off infrequent words to vocab of size nb_words\n",
    "    if nb_words is not None:\n",
    "        x_pos = cap_words(x_pos, nb_words, oov)\n",
    "        x_neg = cap_words(x_neg, nb_words, oov)\n",
    "\n",
    "    # cut off most frequent skip_top words\n",
    "    if skip_top > 0:\n",
    "        x_pos = skip_n(x_pos, skip_top, oov)\n",
    "        x_neg = skip_n(x_neg, skip_top, oov)\n",
    "\n",
    "    # prepend each sequence with start and raise indices by index_from\n",
    "    x_pos = push_indices(x_pos, start, index_from)\n",
    "    x_neg = push_indices(x_neg, start, index_from)\n",
    "    \n",
    "    x_pos = np.array(x_pos)\n",
    "    y_pos = np.array(y_pos)\n",
    "\n",
    "    x_neg = np.array(x_neg)\n",
    "    y_neg = np.array(y_neg)\n",
    "    \n",
    "    return (x_pos, y_pos), (x_neg, y_neg)\n",
    "\n",
    "\n",
    "def load_embeddings(nb_words=None, emb_dim=200, index_from=3,\n",
    "                    vocab='ow3d.dict.pkl', \n",
    "                    w2v='/data/nlp/corpora/twitter4food/food_vectors_clean.txt'):\n",
    "\n",
    "    f = open(vocab, 'rb')\n",
    "    word_index = pkl.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    if nb_words is not None:\n",
    "        max_features = min(nb_words, len(word_index))\n",
    "    else:\n",
    "        max_features = len(word_index)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v, 'rb')\n",
    "    fl = f.readline().strip().decode('UTF-8')\n",
    "    i = 1\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0].decode('UTF-8')\n",
    "        if word in word_index:\n",
    "            if word_index[word] < max_features:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        if i % 1000 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "        if i % 100000 == 0:\n",
    "            print(\"\")\n",
    "            \n",
    "        i = i + 1\n",
    "\n",
    "    f.close()\n",
    "    print(\"\")\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features+index_from, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i+index_from] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "def bootstrap(gold, pred, reps=100000):\n",
    "    '''\n",
    "    # Arguments\n",
    "        gold: list of gold (ground-truth) integer labels\n",
    "        pred: list of predicted integer labels\n",
    "        reps: how many repetitions to do (more=more accurate)\n",
    "\n",
    "    Run a bootstrap significance test. Returns prediction \n",
    "    accuracy (out of 1), the accuracy of the baseline of \n",
    "    choosing the most common label in the gold labels, and \n",
    "    the p-value (the probability that you would do this much \n",
    "    better than the baseline by chance).\n",
    "    '''\n",
    "    accts = len(gold)\n",
    "    hist = {}\n",
    "    hist[-1] = 0\n",
    "    for v in gold:\n",
    "        if v in hist:\n",
    "            hist[v] = hist[v] + 1\n",
    "        else:\n",
    "            hist[v] = 1\n",
    "    baseline = max(hist.values()) / float(accts)\n",
    "    agr = np.array(gold == pred, dtype='int32')\n",
    "    better = np.zeros(reps)\n",
    "    for i in range(reps):\n",
    "        sample = np.random.choice(agr, accts)\n",
    "        if np.mean(sample) > baseline:\n",
    "            better[i] = 1\n",
    "    p = (1. - np.mean(better))\n",
    "    if p < 0.05:\n",
    "        stars = '*'\n",
    "    elif p < 0.01:\n",
    "        stars = '**'\n",
    "    elif p < 0.001:\n",
    "        stars = '***'\n",
    "    else:\n",
    "        stars = ''\n",
    "        \n",
    "    acc = np.mean(agr)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(len(gold)):\n",
    "        if gold[i] == 1 and pred[i] == 1:\n",
    "            tp = tp + 1\n",
    "        elif gold[i] == 1 and pred[i] != 1:\n",
    "            fn = fn + 1\n",
    "        elif gold[i] != 1 and pred[i] == 1:\n",
    "            fp = fp +1\n",
    "        else:\n",
    "            tn = tn +1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1s = []\n",
    "    tps = []\n",
    "    tns = []\n",
    "    fns = []\n",
    "    fps = []\n",
    "    for lbl in (hist.keys()):\n",
    "        if lbl == -1:\n",
    "            continue\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i] == lbl and pred[i] == lbl:\n",
    "                tp = tp + 1\n",
    "            elif gold[i] == lbl and pred[i] != lbl:\n",
    "                fn = fn + 1\n",
    "            elif gold[i] != lbl and pred[i] == lbl:\n",
    "                fp = fp +1\n",
    "            else:\n",
    "                tn = tn +1\n",
    "\n",
    "        tps.append(tp)\n",
    "        tns.append(tn)\n",
    "        fns.append(fn)\n",
    "        fps.append(fp)\n",
    "        if (tp + fp == 0):\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        if (tp + fn == 0):\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec + rec == 0):\n",
    "            f1s.append(0)\n",
    "        else:\n",
    "            f1s.append(2 * prec * rec / (prec + rec))\n",
    "\n",
    "    f1s = np.array(f1s)\n",
    "    tps = np.array(tps)\n",
    "    tns = np.array(tns)\n",
    "    fns = np.array(fns)\n",
    "    fps = np.array(fps)\n",
    "    prec = tps.sum() / (tps.sum() + fps.sum())\n",
    "    rec = tps.sum() / (tps.sum() + fns.sum())\n",
    "    microf1 = 2 * prec * rec / (prec + rec)\n",
    "    macrof1 = f1s.sum() / len(f1s)\n",
    "    \n",
    "    print('accuracy = %.4f' % acc)\n",
    "    print('precision = %.4f' % precision)\n",
    "    print('recall = %.4f' % recall)\n",
    "    print('microF1 = %.4f' % microf1)\n",
    "    print('macroF1 = %.4f' % macrof1)\n",
    "    print('baseline = %.4f' % baseline)\n",
    "    print('p = %.6f%s' % (p, stars))\n",
    "    return (acc, baseline, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and reshape data\n",
    "\n",
    "The data must be loaded, padded, and reshaped to train a tweet-level classifier.\n",
    "Each tweet gets the label of the account (even though most tweets are irrelevant to the classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220 train sequences\n",
      "134 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxtweets = 2000\n",
    "maxlen = 50  # cut texts to this number of words (among top max_features most common words)\n",
    "\n",
    "# These come out shuffled\n",
    "(x_pos, y_pos), (x_neg, y_neg) = load_data(nb_words=max_features, maxlen=maxlen)\n",
    "\n",
    "# length of the test partition\n",
    "pos_len = int(len(y_pos)/10.0)\n",
    "neg_len = int(len(y_neg)/10.0)\n",
    "\n",
    "# This convoluted way of making partitions assures equal pos and neg labels per partition\n",
    "pos_test_ids = list(range(pos_len))\n",
    "neg_test_ids = list(range(neg_len))\n",
    "\n",
    "pos_train_ids = list(range(pos_len, len(y_pos)))\n",
    "neg_train_ids = list(range(neg_len, len(y_neg)))\n",
    "\n",
    "X_train = np.append(x_pos[pos_train_ids], x_neg[neg_train_ids])\n",
    "y_train = np.append(y_pos[pos_train_ids], y_neg[neg_train_ids])\n",
    "\n",
    "X_test = np.append(x_pos[pos_test_ids], x_neg[neg_test_ids])\n",
    "y_test = np.append(y_pos[pos_test_ids], y_neg[neg_test_ids])\n",
    "\n",
    "X_train, y_train = shuffle_in_unison(X_train, y_train)\n",
    "X_test, y_test = shuffle_in_unison(X_test, y_test)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1220, 2000, 50)\n",
      "X_test shape: (134, 2000, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad3d(X_train, maxtweets=maxtweets, maxlen=maxlen)\n",
    "X_test = pad3d(X_test, maxtweets=maxtweets, maxlen=maxlen)\n",
    "train_shp = X_train.shape\n",
    "test_shp = X_test.shape\n",
    "print('X_train shape:', train_shp)\n",
    "print('X_test shape:', test_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(train_shp[0] * train_shp[1], train_shp[2])\n",
    "y_train_flat = y_train.repeat(train_shp[1])\n",
    "X_train_shuff, y_train_shuff = shuffle_in_unison(X_train_flat, y_train_flat)\n",
    "\n",
    "X_test_flat = X_test.reshape(test_shp[0] * test_shp[1], test_shp[2])\n",
    "y_test_flat = y_test.repeat(test_shp[1])\n",
    "\n",
    "# We shuffle the flattened reps. for better training\n",
    "# (but keep the original order for our by-account classification)\n",
    "X_test_shuff, y_test_shuff = shuffle_in_unison(X_test_flat, y_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just clearing up space -- from now on, we use the flattened representations\n",
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................................\n",
      "Found 18570 word vectors.\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 200\n",
    "embeddings = load_embeddings(nb_words=max_features, emb_dim=emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_filter = 64 # how many convolutional filters\n",
    "filter_length = 5 # how many tokens a convolution covers\n",
    "pool_length = 4 # how many cells of convolution to pool across when maxing\n",
    "nb_epoch = 1 # how many training epochs\n",
    "batch_size = 256 # how many tweets to train at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build first model (tweet-level)...\n"
     ]
    }
   ],
   "source": [
    "print('Build first model (tweet-level)...')\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(max_features + 3, \n",
    "                     emb_dim, \n",
    "                     input_length=maxlen,\n",
    "                     weights=[embeddings]\n",
    "                    ))#, \n",
    "                     #mask_zero=True))\n",
    "model1.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "model1.add(MaxPooling1D(pool_length=pool_length))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 50, 200)       4000600     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 46, 64)        64064       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 11, 64)        0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 704)           0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           90240       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             129         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,155,033\n",
      "Trainable params: 4,155,033\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "model1.fit(X_train_shuff, y_train_shuff, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "           validation_data=(X_test_shuff, y_test_shuff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save('tweet_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = load_model('tweet_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, acc = model1.evaluate(X_test_flat, y_test_flat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model1.predict(X_test_flat)\n",
    "pred = pred.reshape((test_shp[0], test_shp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# account classification with each tweet's classification getting an equal vote\n",
    "predmn = np.mean(pred, axis=1)\n",
    "predmn = (predmn >= 0.5).astype(int)\n",
    "\n",
    "# weight by recency (most recent tweets first)\n",
    "wts = np.linspace(1., 0.01, 2000)\n",
    "predwm = np.average(pred, axis=1, weights=wts)\n",
    "predwm = (predwm >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = y_test.flatten()\n",
    "\n",
    "print('Unweighted mean')\n",
    "bootstrap(y, predmn)\n",
    "\n",
    "print('\\nWeighted mean')\n",
    "bootstrap(y, predwm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate data structure\n",
    "\n",
    "Having trained a tweet-level classifier with `model1`, we now create an identical (trained) net except that we cut off final, classifying layer. This allows us to pass forward a 128-length vector for each tweet. The tweets will then be grouped by account (there being a fixed number of tweets per account). The resulting 2-D structure can be passed to a GRU or LSTM for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate = Sequential()\n",
    "intermediate.add(Embedding(max_features + 3, \n",
    "                     emb_dim, \n",
    "                     input_length=maxlen,\n",
    "                     weights=[embeddings]\n",
    "                    ))#, \n",
    "                     #mask_zero=True))\n",
    "intermediate.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "intermediate.add(MaxPooling1D(pool_length=pool_length))\n",
    "intermediate.add(Flatten())\n",
    "intermediate.add(Dense(128))\n",
    "intermediate.add(Activation('relu'))\n",
    "\n",
    "for l in range(len(intermediate.layers)):\n",
    "    intermediate.layers[l].set_weights(model1.layers[l].get_weights())\n",
    "    intermediate.layers[l]\n",
    "\n",
    "intermediate.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_mid = K.eval(intermediate(K.variable(X_test_flat)))\n",
    "X_test_mid = X_test_mid.reshape((test_shp[0], test_shp[1], 128))\n",
    "X_test_mid = np.fliplr(X_test_mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU classification\n",
    "\n",
    "This is the second part of the net, which takes in the 2D account representation and returns an account classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(GRU(128, \n",
    "               dropout_W=0.2, \n",
    "               dropout_U=0.2,\n",
    "               input_shape=(X_test_mid.shape[1], X_test_mid.shape[2])))\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk = 256\n",
    "X_train_mid = np.zeros((train_shp[0], train_shp[1], 128))\n",
    "for i in range(0, train_shp[0], chunk):\n",
    "    last_idx = min(chunk, train_shp[0] - i)\n",
    "    print('accounts ' + str(i) + ' through ' + str(i + last_idx))\n",
    "    X_train_chunk = K.eval(intermediate(K.variable(X_train_flat[i * maxtweets : (i + last_idx) * maxtweets])))\n",
    "    X_train_chunk = X_train_chunk.reshape((last_idx, maxtweets, 128))\n",
    "    X_train_chunk = np.fliplr(X_train_chunk)\n",
    "    X_train_mid[i:(i + last_idx)] = X_train_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.fit(X_train_mid,  \n",
    "           y_train, \n",
    "           batch_size=batch_size,\n",
    "           nb_epoch=1,\n",
    "           validation_data=(X_test_mid, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, acc = model2.evaluate(X_test_mid, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 convolutions\n",
    "\n",
    "It is possible in some instances to get more or better information from multiple convolution sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, MaxPooling1D, Flatten, merge\n",
    "\n",
    "# fewer filters because we have 3x the convolution stacks\n",
    "nb_filters = 64\n",
    "batch_size = 256\n",
    "\n",
    "tweet = Input(shape=(50,), dtype='int32')\n",
    "embed = Embedding(max_features + 3, \n",
    "                            emb_dim, \n",
    "                            weights=[embeddings]\n",
    "                           )(tweet) #,\n",
    "                     #mask_zero=True))\n",
    "\n",
    "conv3 = Convolution1D(nb_filter=nb_filters,\n",
    "                      filter_length=3,\n",
    "                      border_mode=\"same\",\n",
    "                      activation='relu')(embed)\n",
    "conv3 = MaxPooling1D(pool_length= maxlen - 3 + 1)(conv3)\n",
    "conv3 = Flatten()(conv3)\n",
    "\n",
    "conv4 = Convolution1D(nb_filter=nb_filters,\n",
    "                      filter_length=4,\n",
    "                      border_mode=\"same\",\n",
    "                      activation='relu')(embed)\n",
    "conv4 = MaxPooling1D(pool_length= maxlen - 4 + 1)(conv4)\n",
    "conv4 = Flatten()(conv4)\n",
    "\n",
    "conv5 = Convolution1D(nb_filter=nb_filters,\n",
    "                      filter_length=5,\n",
    "                      border_mode=\"same\",\n",
    "                      activation='relu')(embed)\n",
    "conv5 = MaxPooling1D(pool_length= maxlen - 5 + 1)(conv5)\n",
    "conv5 = Flatten()(conv5)\n",
    "\n",
    "# wider dense layer because more convolutions\n",
    "dense_wide = Dense(256)(merge([conv3, conv4, conv5], mode='concat', concat_axis=1))\n",
    "relu = Activation('relu')(dense_wide)\n",
    "dropout = Dropout(0.4)(relu)\n",
    "dense_narrow = Dense(1)(dropout)\n",
    "output = Activation('sigmoid')(dense_narrow)\n",
    "\n",
    "towers = Model(input=tweet, output=output)\n",
    "towers.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "towers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "towers.fit(X_train_shuff, y_train_shuff, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "           validation_data=(X_test_shuff, y_test_shuff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, acc = towers.evaluate(X_test_shuff, y_test_shuff,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
